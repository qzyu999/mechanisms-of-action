{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "\n",
    "pd.set_option('display.max_rows', None) # Show max rows/columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "<a id=\"top\"></a>\n",
    "\n",
    "1.\t[Load Data](#load_data)\n",
    "2.\t[EDA](#eda)\n",
    "    + 2.1 [Target](#target)\n",
    "    + 2.2 [Features](#features)\n",
    "        + 2.2.1 [Categorical Features](#cat_feat)\n",
    "        + 2.2.2 [Numeric Features](#num_feat)\n",
    "3.\t[Code Test](#test)\n",
    "4.  [OVR](#ovr)\n",
    "    + 4.1 [Test a model](#test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "<a id=\"load_data\"></a>\n",
    "<a href=\"#top\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../input/train_features.csv')\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('../input/train_targets_scored.csv')\n",
    "print(y.shape)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('../input/test_features.csv')\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_csv('../input/sample_submission.csv')\n",
    "print(y_test.shape)\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA\n",
    "<a id=\"eda\"></a>\n",
    "<a href=\"#top\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23,814 examples, 875 features (excluding the unique identifier), 206 class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X, y], axis=1).head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...\n",
    "train_features = pd.read_csv('../input/train_features.csv')\n",
    "train_labels_ohe = pd.read_csv('../input/train_targets_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.drop(df.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the OHE labels\n",
    "y = train_labels_ohe.iloc[:,1:].idxmax(axis=1)\n",
    "y = pd.DataFrame(y)\n",
    "y.columns = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_features, y], axis=1) # Recombine into single df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Target\n",
    "<a id=\"target\"></a>\n",
    "<a href=\"#top\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/38334296/reversing-one-hot-encoding-in-pandas\n",
    "y_target_col = y.iloc[:,1:].idxmax(axis=1) # Reverse the OHE labels\n",
    "y_target_col = pd.DataFrame(y_target_col)\n",
    "y_target_col.columns = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_value_counts = y_target_col.value_counts()\n",
    "y_value_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency plot of the 206 class labels shows that it's highly imbalanced, with a large skew towards \"5-alpha_reductase_inhibitor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/46623583/seaborn-countplot-order-categories-by-count\n",
    "plt.figure(figsize=(100,20))\n",
    "ax = sns.countplot(x=\"label\",\n",
    "                   data=y_target_col,\n",
    "                   order = y_target_col['label'].value_counts().index)\n",
    "_ = ax.set(xlabel=\"Mechanism of Action (MoA)\", ylabel = \"Frequency\")\n",
    "_ = ax.set_title('Frequency Histogram of MoA')\n",
    "# Reference: https://www.drawingfromdata.com/how-to-rotate-axis-labels-in-seaborn-and-matplotlib\n",
    "_ = plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='x-large'  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different labels all have various frequencies. The dataset is obviously quite imbalanced. The next step is to create a frequency plot. It will be split amongst two groups: <100 and between 100;1,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_value_counts_under100 = y_value_counts[y_value_counts < 100]\n",
    "y_value_counts_under100 = pd.DataFrame(y_value_counts_under100)\n",
    "y_value_counts_under100.reset_index(level=0, inplace=True)\n",
    "y_value_counts_under100 = y_value_counts_under100['label'].tolist()\n",
    "y_value_counts_under100 = y_target_col[y_target_col['label'].isin(y_value_counts_under100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/46623583/seaborn-countplot-order-categories-by-count\n",
    "plt.figure(figsize=(150,50))\n",
    "ax = sns.countplot(x=\"label\",\n",
    "                   data=y_value_counts_under100,\n",
    "                   order = y_value_counts_under100['label'].value_counts().index)\n",
    "_ = ax.set(xlabel=\"Mechanism of Action (MoA)\", ylabel = \"Frequency\")\n",
    "_ = ax.set_title('Frequency Histogram of MoA (Count < 100)')\n",
    "# Reference: https://www.drawingfromdata.com/how-to-rotate-axis-labels-in-seaborn-and-matplotlib\n",
    "_ = plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='x-large'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_value_counts_100to1000 = y_value_counts[(y_value_counts >= 100) & (y_value_counts < 1000)]\n",
    "y_value_counts_100to1000 = pd.DataFrame(y_value_counts_100to1000)\n",
    "y_value_counts_100to1000.reset_index(level=0, inplace=True)\n",
    "y_value_counts_100to1000 = y_value_counts_100to1000['label'].tolist()\n",
    "y_value_counts_100to1000 = y_target_col[y_target_col['label'].isin(y_value_counts_100to1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/46623583/seaborn-countplot-order-categories-by-count\n",
    "plt.figure(figsize=(10,8))\n",
    "ax = sns.countplot(x=\"label\",\n",
    "                   data=y_value_counts_100to1000,\n",
    "                   order = y_value_counts_100to1000['label'].value_counts().index)\n",
    "_ = ax.set(xlabel=\"Mechanism of Action (MoA)\", ylabel = \"Frequency\")\n",
    "_ = ax.set_title('Frequency Histogram of MoA (100 <= Count < 1,000)')\n",
    "# Reference: https://www.drawingfromdata.com/how-to-rotate-axis-labels-in-seaborn-and-matplotlib\n",
    "_ = plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='x-large'  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Features\n",
    "<a id=\"features\"></a>\n",
    "<a href=\"#top\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes.head(8) # most are all numeric, only the sig_id, cp_type, and cp_dose are categorical\n",
    "# cp_time can be considered categorical also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Categorical Features\n",
    "<a id=\"cat_feat\"></a>\n",
    "<a href=\"#top\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_type = X.iloc[:,1]\n",
    "cp_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.countplot(x='cp_type', data=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_time = X.iloc[:,2]\n",
    "cp_time.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.countplot(x='cp_time', data=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_dose = X.iloc[:,3]\n",
    "cp_dose.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.countplot(x='cp_dose', data=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Numerical Features\n",
    "<a id=\"num_feat\"></a>\n",
    "<a href=\"#top\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_describe = X.iloc[:,1:].describe() # statistics on the numeric attributes\n",
    "X_describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all numeric features have some variance, i.e., none are constant vectors with single values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(X_describe.loc['std',:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split g- and c- features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code Test\n",
    "<a id=\"test\"></a>\n",
    "<a href=\"#top\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "The issue is trying to figure out a working pipeline to go from the given dataset and output the predictions to kaggle. Normally, this sort of step is not required, but since we need to submit it to kaggle that means that there needs to be some small steps taken to ensure that the correct files are going back and forth.\n",
    "\n",
    "We are given the kaggle data, which basically has the data split by feature/target and train/test. So, for the training data, there are:\n",
    "\n",
    "23,814 examples, 875 features (excluding the unique identifier), 206 class labels (excluding the unique identifier)\n",
    "\n",
    "For the testing data, there are:\n",
    "\n",
    "3,982 examples, 875 features (excluding the unique identifier), 206 class labels (excluding the unique identifier)\n",
    "\n",
    "In the typical process, the target vector is a single $n\\times 1$ vector. It is possible to convert the 206 columns into a single column with the class labels as values. However, they're already in a OHE state.\n",
    "\n",
    "Possibilities:\n",
    "1. Convert to single target vector. Use the pipeline to re-OHE the class labels. Likewise, OHE the categorical variables. This process is important for using sk-learn, since the library seems to prefer having the single target vector rather than a provided OHE set of columns.\n",
    "    - A difficulty with this however is that the data needs to also be split into K folds for k-fold CV.\n",
    "        - Another issue is that each of the splits will not have all the classes.\n",
    "        - However, it also must be such that 5-fold CV is used to tune the model, then a final test model is used based on those parameters to output a set of labels.\n",
    "        \n",
    "Cross-Validation stage:\n",
    "- Perform 5-fold CV for a simple logistic regression model in sklearn.\n",
    "- Find the tuned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try a simple approach first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/train_features.csv')\n",
    "train_labels_ohe = pd.read_csv('../input/train_targets_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.drop(train_features.columns[0], axis=1,inplace=True)\n",
    "train_labels_ohe.drop(train_labels_ohe.columns[0], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import model_selection\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names\n",
    "train_features_col_names = train_features.columns.tolist()\n",
    "\n",
    "cat_cols = ['cp_type', 'cp_time', 'cp_dose'] # Identify categorical columns\n",
    "\n",
    "ohe = OneHotEncoder() # Load OHE\n",
    "\n",
    "# Get the column names after OHE\n",
    "# Reference: https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder\n",
    "_ = ohe.fit_transform(train_features[cat_cols])\n",
    "ohe_names = ohe.get_feature_names(cat_cols)\n",
    "ohe_names = ohe_names.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), list(range(0,3)))], remainder='passthrough')\n",
    "train_features = ct.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.DataFrame(train_features)\n",
    "train_features.columns = ohe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the OHE labels\n",
    "y = train_labels_ohe.iloc[:,:].idxmax(axis=1)\n",
    "y = pd.DataFrame(y)\n",
    "y.columns = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_features, y], axis=1) # Recombine into single df\n",
    "\n",
    "df['kfold'] = -1 # Create k-folds column\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True) # Randomize the dataset\n",
    "\n",
    "y = df.target.values # Subset the target column\n",
    "\n",
    "# Initialize the stratified k-fold module from sklearn\n",
    "kf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,7:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end simple approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test log reg\n",
    "\n",
    "### Check for if the kfold column needs to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logres.py\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import config\n",
    "\n",
    "fold=0\n",
    "# def run(fold):\n",
    "# Read the data\n",
    "df = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "# Separate into train and validation\n",
    "df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "# Drop the target and and convert to numpy\n",
    "x_train = df_train.drop(['target', 'kfold'], axis=1).values\n",
    "y_train = df_train.target.values\n",
    "\n",
    "# Repeat for validation data\n",
    "x_valid = df_valid.drop(['target', 'kfold'], axis=1).values\n",
    "y_valid = df_valid.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logres.py\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import config\n",
    "\n",
    "fold=0\n",
    "# def run(fold):\n",
    "# Read the data\n",
    "df = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "# Separate into train and validation\n",
    "df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "# Drop the target and and convert to numpy\n",
    "x_train = df_train.drop('target', axis=1).values\n",
    "y_train = df_train.target.values\n",
    "\n",
    "# Repeat for validation data\n",
    "x_valid = df_valid.drop('target', axis=1).values\n",
    "y_valid = df_valid.target.values\n",
    "\n",
    "# Apply feature scaling to the numeric attributes\n",
    "sc = StandardScaler()\n",
    "x_train[:,7:] = sc.fit_transform(x_train[:,7:])\n",
    "x_valid[:,7:] = sc.transform(x_valid[:,7:])\n",
    "\n",
    "# Intiialize the classifier\n",
    "model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions\n",
    "valid_preds = model.predict_proba(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue, the predictions are missing 2 columns 204 rather than 206. The validation set includes 203 classes, while the train set includes 204 classes.\n",
    "\n",
    "It would make sense that the y_preds are based on the classifier that has seen a number of classes equal to the number of classes in the training set. However, in trying to find the log-loss, the number of class predictions per observation is in this case larger than the number of classes in the validation set. Therefore, the log_loss function encounters this error.\n",
    "\n",
    "A goal now is to find the associated labels with y_pred from y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(y_valid)), len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the accuracy\n",
    "log_loss_score = metrics.log_loss(y_valid, valid_preds, labels=model.classes_)\n",
    "print(f\"Fold={fold}, Log-Loss={log_loss_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the labels=classes_ with iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = list(range(0,40))\n",
    "l2 = list(range(50, 90))\n",
    "l3 = list(range(100,140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.extend(l2)\n",
    "l1.extend(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[l1,:]\n",
    "y_train = y[l1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4 = list(range(40,50))\n",
    "l5 = list(range(90,100))\n",
    "l4.extend(l5)\n",
    "X_valid = X[l4,:]\n",
    "y_valid = y[l4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds = model.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_score = metrics.log_loss(y_valid, valid_preds)\n",
    "print(f\"Fold={fold}, Log-Loss={log_loss_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_score = metrics.log_loss(y_valid, valid_preds, labels=model.classes_)\n",
    "print(f\"Fold={fold}, Log-Loss={log_loss_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end log reg test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "import config\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "# TRAINING_FILE = \"../input/train_folds.csv\"\n",
    "\n",
    "# MODEL_OUTPUT = \"../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "# Separate into train and validation\n",
    "df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "# Drop the target and and convert to numpy\n",
    "x_train = df_train.drop('target', axis=1).values\n",
    "y_train = df_train.target.values\n",
    "\n",
    "# Repeat for validation data\n",
    "x_valid = df_valid.drop('target', axis=1).values\n",
    "y_valid = df_valid.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiialize the classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Create predictions\n",
    "preds = clf.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    # Read the data\n",
    "    df = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "    # Separate into train and validation\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    # Drop the target and and convert to numpy\n",
    "    x_train = df_train.drop('target', axis=1).values\n",
    "    y_train = df_train.target.values\n",
    "\n",
    "    # Repeat for validation data\n",
    "    x_valid = df_valid.drop('target', axis=1).values\n",
    "    y_valid = df_valid.target.values\n",
    "\n",
    "    # Intiialize the classifier\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Create predictions\n",
    "    preds = clf.predict(x_valid)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    accuracy = metrics.accuracy_score(y_valid, preds)\n",
    "    print(f\"Fold={fold}, Accuracy={accuracy}\")\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(\n",
    "        clf,\n",
    "        os.path.join(config.MODEL_OUTPUT, f\"dt_{fold}.bin\")\n",
    "    )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize the argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     # Add arguments to parser\n",
    "#     parser.add_argument(\n",
    "#         \"--fold\",\n",
    "#         type=int\n",
    "#     )\n",
    "#     args = parser.parse_args() # Read arguments from command line\n",
    "\n",
    "#     run(fold=args.fold) # Run the folds specified in the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/train_features.csv')\n",
    "train_labels_ohe = pd.read_csv('../input/train_targets_scored.csv')\n",
    "\n",
    "# Drop the unique key column\n",
    "train_features.drop(train_features.columns[0], axis=1,inplace=True)\n",
    "train_features_col_names = train_features.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n",
    "\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "_ = ohe.fit_transform(train_features[cat_cols])\n",
    "ohe_names = ohe.get_feature_names(cat_cols)\n",
    "ohe_names = ohe_names.tolist()\n",
    "columns_trans = make_column_transformer(\n",
    "        (preprocessing.OneHotEncoder(),\n",
    "        cat_cols),\n",
    "        remainder='passthrough')\n",
    "train_features = columns_trans.fit_transform(train_features)\n",
    "\n",
    "train_features_col_names = [col for col in train_features_col_names if col not in cat_cols]\n",
    "\n",
    "ohe_names.extend(train_features_col_names)\n",
    "\n",
    "train_features = pd.DataFrame(train_features)\n",
    "train_features.columns = ohe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logres.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    # Read the data\n",
    "    df = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "    # Separate into train and validation\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    # Drop the target and and convert to numpy\n",
    "    x_train = df_train.drop('target', axis=1).values\n",
    "    y_train = df_train.target.values\n",
    "\n",
    "    # Repeat for validation data\n",
    "    x_valid = df_valid.drop('target', axis=1).values\n",
    "    y_valid = df_valid.target.values\n",
    "\n",
    "    # Intiialize the classifier\n",
    "    model = linear_model.LogisticRegression()\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Create predictions\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    auc = metrics.roc_auc_score(y_valid, valid_preds)\n",
    "    print(f\"Fold={fold}, AUC={auc}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "for fold_ in range(5):\n",
    "    run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=0\n",
    "# def run(fold):\n",
    "# Read the data\n",
    "df = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "# Separate into train and validation\n",
    "df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "# Drop the target and and convert to numpy\n",
    "x_train = df_train.drop('target', axis=1).values\n",
    "y_train = df_train.target.values\n",
    "\n",
    "# Repeat for validation data\n",
    "x_valid = df_valid.drop('target', axis=1).values\n",
    "y_valid = df_valid.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiialize the classifier\n",
    "model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Create predictions\n",
    "# valid_preds = model.predict_proba(x_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds = model.predict_proba(x_valid)\n",
    "valid_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_metric = metrics.log_loss(y_valid, valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and print the accuracy\n",
    "auc = metrics.roc_auc_score(y_valid, valid_preds)\n",
    "print(f\"Fold={fold}, AUC={auc}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# for fold_ in range(5):\n",
    "# run(fold_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test sample submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../input/train_features.csv')\n",
    "y = pd.read_csv('../input/train_targets_scored.csv')\n",
    "\n",
    "# Drop the unique key column\n",
    "X.drop(X.columns[0], axis=1,inplace=True)\n",
    "y.drop(y.columns[0], axis=1, inplace=True)\n",
    "X_col_names = X.columns.tolist()\n",
    "\n",
    "cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n",
    "\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "_ = ohe.fit_transform(X[cat_cols])\n",
    "ohe_names = ohe.get_feature_names(cat_cols)\n",
    "ohe_names = ohe_names.tolist()\n",
    "columns_trans = make_column_transformer(\n",
    "        (preprocessing.OneHotEncoder(),\n",
    "        cat_cols),\n",
    "        remainder='passthrough')\n",
    "X = columns_trans.fit_transform(X)\n",
    "\n",
    "X_col_names = [col for col in X_col_names if col not in cat_cols]\n",
    "\n",
    "ohe_names.extend(X_col_names)\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = ohe_names\n",
    "\n",
    "# Reverse the OHE labels\n",
    "y = y.idxmax(axis=1)\n",
    "y = pd.DataFrame(y)\n",
    "y.columns = ['target']\n",
    "\n",
    "df = pd.concat([X, y], axis=1) # Recombine into single df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop('target', axis=1).values\n",
    "y_train = df.target.values\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train[:,7:] = sc.fit_transform(X_train[:,7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_score = metrics.log_loss(y_valid, test_preds,\n",
    "    labels=model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('../input/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.drop(X_test.columns[0], axis=1, inplace=True) # Drop the unique key column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['cp_type', 'cp_time', 'cp_dose'] # Identify categorical columns\n",
    "\n",
    "ohe = OneHotEncoder() # Load OHE\n",
    "\n",
    "# Get the column names after OHE\n",
    "# Reference: https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder\n",
    "_ = ohe.fit_transform(X_test[cat_cols])\n",
    "ohe_names = ohe.get_feature_names(cat_cols)\n",
    "ohe_names = ohe_names.tolist()\n",
    "\n",
    "# Fix new column names to include OHE names and normal feature names\n",
    "X_col_names = [col for col in X_col_names if col\\\n",
    "    not in cat_cols]\n",
    "ohe_names.extend(X_col_names)\n",
    "\n",
    "# Transform the data with OHE on the indices of the cat variables\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('encoder', OneHotEncoder(), list(range(0,3)))],\n",
    "    remainder='passthrough')\n",
    "X_test = ct.fit_transform(X_test)\n",
    "# X = pd.DataFrame(ct.fit_transform(X))\n",
    "# X.columns = ohe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_test[:,7:] = sc.fit_transform(X_test[:,7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. OVR\n",
    "<a id=\"ovr\"></a>\n",
    "<a href=\"#top\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to output a simple baseline model that implements the OVR strategy. The problem is that the dataset is multilabel and there are many classes with a great degree of sparsity. An idea is to choose some arbitrary cutoff point (e.g. 300) to limit the number of classes to test and those to 'ignore.' The reason is that with so few examples, that it could be difficult or impossible to develop a model that can accurately predict those classes. Therefore, using $c<k$, where $c$ is the number of chosen classes and $k$ is the number of total classes, we can create an OVR approach to create $c$ groups of models that are binary classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start test on ovr_method.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ovr_method.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# import config\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    # Drop the target and and convert to numpy\n",
    "    x_train = df_train.drop(['target', 'kfold'], axis=1).values\n",
    "    y_train = df_train.target.values\n",
    "\n",
    "    # Repeat for validation data\n",
    "    x_valid = df_valid.drop(['target', 'kfold'], axis=1).values\n",
    "    y_valid = df_valid.target.values\n",
    "\n",
    "    # Apply feature scaling to the numeric attributes\n",
    "    sc = StandardScaler()\n",
    "    x_train[:,7:] = sc.fit_transform(x_train[:,7:])\n",
    "    x_valid[:,7:] = sc.transform(x_valid[:,7:])\n",
    "\n",
    "    # Intiialize the classifier\n",
    "    model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Create predictions\n",
    "    y_pred_probs = model.predict_proba(x_valid)\n",
    "    y_preds = model.predict(x_valid)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    log_loss_score = metrics.log_loss(y_valid, y_pred_probs,\n",
    "        labels=model.classes_)\n",
    "    auc = metrics.roc_auc_score(y_valid, y_preds,\n",
    "        labels=model.classes_)\n",
    "    accuracy = metrics.accuracy_score(y_valid, y_preds)\n",
    "    \n",
    "    print(f\"Fold={fold}, Log-Loss={log_loss_score}, AUC={auc}, Accuracy={accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create OVR target vectors\n",
    "# X = pd.read_csv('../input/train_features.csv')\n",
    "y = pd.read_csv('../input/train_targets_scored.csv')\n",
    "\n",
    "# Add hidden class\n",
    "zero_class_indices = y[y.iloc[:,1:].apply(sum, axis=1) == 0].index\n",
    "y['hidden_class'] = 0\n",
    "y['hidden_class'].iloc[zero_class_indices] = 1\n",
    "\n",
    "class_counts = y.iloc[:,1:].sum(axis=0)\n",
    "class_counts = class_counts.sort_values(ascending=False)\n",
    "class_counts_sub = class_counts.head(13)\n",
    "retained_classes = class_counts_sub.index.values\n",
    "y2 = y.iloc[:,1:]\n",
    "\n",
    "### The following creates 'c' binary target vectors saved in a list: 'binary_vector_list'\n",
    "class_index_list = [] # Save indices that contain the class\n",
    "for c in retained_classes:\n",
    "    c_indices = y2.loc[:,c][y2.loc[:,c] == 1].index.values\n",
    "    class_index_list.append([c, c_indices])\n",
    "\n",
    "binary_vector_list = []\n",
    "n = len(y)\n",
    "for i in class_index_list: # Loop through class/index pairs\n",
    "    zeros = [0] * n\n",
    "    for j in range(n): # Loop through all rows\n",
    "        # Check if the index should be one instead\n",
    "        if j in i[1]:\n",
    "            zeros[j] = 1\n",
    "    binary_vector_list.append(pd.DataFrame({i[0]: zeros}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loop through OVR classes\n",
    "for i in binary_vector_list:\n",
    "    y_temp = copy.deepcopy(i)\n",
    "    class_name = y_temp.columns[0]\n",
    "    X = pd.read_csv('../input/train_features.csv')\n",
    "    X.drop(X.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    # Save the column names\n",
    "    X_col_names = X.columns.tolist()\n",
    "\n",
    "    cat_cols = ['cp_type', 'cp_time', 'cp_dose'] # Identify categorical columns\n",
    "\n",
    "    ohe = OneHotEncoder() # Load OHE\n",
    "\n",
    "    # Get the column names after OHE\n",
    "    # Reference: https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder\n",
    "    _ = ohe.fit_transform(X[cat_cols])\n",
    "    ohe_names = ohe.get_feature_names(cat_cols)\n",
    "    ohe_names = ohe_names.tolist()\n",
    "\n",
    "    # Fix new column names to include OHE names and normal feature names\n",
    "    X_col_names = [col for col in X_col_names if col\\\n",
    "        not in cat_cols]\n",
    "    ohe_names.extend(X_col_names)\n",
    "\n",
    "    # Transform the data with OHE on the indices of the cat variables\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[('encoder', OneHotEncoder(), list(range(0,3)))],\n",
    "        remainder='passthrough')\n",
    "    X = pd.DataFrame(ct.fit_transform(X))\n",
    "    X.columns = ohe_names\n",
    "    \n",
    "    y_temp.columns = ['target']\n",
    "    \n",
    "    df = pd.concat([X, y_temp], axis=1) # Recombine into single df\n",
    "\n",
    "    df['kfold'] = -1 # Create k-folds column\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True) # Randomize the dataset\n",
    "\n",
    "    y = df.target.values # Subset the target column\n",
    "\n",
    "    # Initialize the stratified k-fold module from sklearn\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Fill the 'kfold' column with the assigned folds\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "        \n",
    "    print(class_name)\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "# Drop the target and and convert to numpy\n",
    "x_train = df_train.drop(['target', 'kfold'], axis=1).values\n",
    "y_train = df_train.target.values\n",
    "\n",
    "# Repeat for validation data\n",
    "x_valid = df_valid.drop(['target', 'kfold'], axis=1).values\n",
    "y_valid = df_valid.target.values\n",
    "\n",
    "# Apply feature scaling to the numeric attributes\n",
    "sc = StandardScaler()\n",
    "x_train[:,7:] = sc.fit_transform(x_train[:,7:])\n",
    "x_valid[:,7:] = sc.transform(x_valid[:,7:])\n",
    "\n",
    "# Intiialize the classifier\n",
    "model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Create predictions\n",
    "y_pred_probs = model.predict_proba(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs.shape, nonscored_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonscored_targets = pd.read_csv(\"../input/train_targets_nonscored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonscored_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ovr_method.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# import config\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_X():\n",
    "    \"\"\"The preprocess_X() function will do the initial preprocessing for the\n",
    "        dataset features.\n",
    "    \"\"\"\n",
    "    X = pd.read_csv(\"../input/train_features.csv\")\n",
    "    X_test = pd.read_csv(\"../input/test_features.csv\")\n",
    "    X.drop(X.columns[0], axis=1, inplace=True)\n",
    "    X_test.drop(X_test.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    # Save the column names\n",
    "    X_col_names = X.columns.tolist()\n",
    "    cat_cols = ['cp_type', 'cp_time', 'cp_dose'] # Identify categorical columns\n",
    "    ohe = OneHotEncoder() # Load OHE\n",
    "    _ = ohe.fit_transform(X[cat_cols])\n",
    "    ohe_names = ohe.get_feature_names(cat_cols)\n",
    "    ohe_names = ohe_names.tolist()\n",
    "\n",
    "    # Fix new column names to include OHE names and normal feature names\n",
    "    X_col_names = [col for col in X_col_names if col\\\n",
    "        not in cat_cols]\n",
    "    ohe_names.extend(X_col_names)\n",
    "\n",
    "    # Transform the data with OHE on the indices of the cat variables\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[('encoder', OneHotEncoder(), list(range(0,3)))],\n",
    "        remainder='passthrough')\n",
    "    X = pd.DataFrame(ct.fit_transform(X))\n",
    "    X.columns = ohe_names\n",
    "    ### Unsure if this is correct ct.transform()\n",
    "    X_test = pd.DataFrame(ct.transform(X_test))\n",
    "    X_test.columns = ohe_names\n",
    "\n",
    "    # Apply feature scaling to the numeric attributes\n",
    "    sc = StandardScaler()\n",
    "    X = X.values\n",
    "    X_test = X_test.values\n",
    "    X[:,7:] = sc.fit_transform(X[:,7:])\n",
    "    X_test[:,7:] = sc.transform(X_test[:,7:])\n",
    "    \n",
    "    return X, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_OVR_targets():\n",
    "    \"\"\"Generate the list of binary OVR target vectors that will be tested.\n",
    "    \"\"\"\n",
    "    y = pd.read_csv(\"../input/train_targets_scored.csv\")\n",
    "\n",
    "    # Add hidden class\n",
    "    zero_class_indices = y[y.iloc[:,1:].apply(sum, axis=1) == 0].index\n",
    "    y['hidden_class'] = 0\n",
    "    y['hidden_class'].iloc[zero_class_indices] = 1\n",
    "    \n",
    "    class_counts = y.iloc[:,1:].sum(axis=0)\n",
    "    class_counts = class_counts.sort_values(ascending=False)\n",
    "    \n",
    "    ### Hard coded # of classes\n",
    "    \n",
    "    class_counts_sub = class_counts.head(13)\n",
    "    retained_classes = class_counts_sub.index.values\n",
    "    y2 = y.iloc[:,1:]\n",
    "\n",
    "    ### The following creates 'c' binary target vectors saved in a list: 'binary_vector_list'\n",
    "    class_index_list = [] # Save indices that contain the class\n",
    "    for c in retained_classes:\n",
    "        c_indices = y2.loc[:,c][y2.loc[:,c] == 1].index.values\n",
    "        class_index_list.append([c, c_indices])\n",
    "\n",
    "    binary_vector_list = []\n",
    "    n = len(y)\n",
    "    for i in class_index_list: # Loop through class/index pairs\n",
    "        zeros = [0] * n\n",
    "        for j in range(n): # Loop through all rows\n",
    "            # Check if the index should be one instead\n",
    "            if j in i[1]:\n",
    "                zeros[j] = 1\n",
    "        binary_vector_list.append(pd.DataFrame({i[0]: zeros}))\n",
    "\n",
    "    return binary_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_targets = generate_OVR_targets()\n",
    "\n",
    "# Edit to use it for sample submission\n",
    "nonscored_targets = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "nonscored_targets.replace(0.5, 0, inplace=True)\n",
    "nonscored_targets['hidden_class'] = 0\n",
    "\n",
    "### Can OHE and standard scale the X_train first\n",
    "X_train, X_test = preprocess_X()\n",
    "\n",
    "for i in ovr_targets:\n",
    "    y_temp = copy.deepcopy(i)\n",
    "    class_name = y_temp.columns[0]\n",
    "\n",
    "    # Intiialize the classifier\n",
    "### Need to later check the correct model for a given feature\n",
    "    model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_temp.values.ravel())\n",
    "\n",
    "    # Create predictions\n",
    "    y_pred_probs = model.predict_proba(X_test)\n",
    "\n",
    "    # Update predicted probabilities\n",
    "    nonscored_targets.loc[:,class_name] = y_pred_probs[:,1]\n",
    "\n",
    "# Go through each row and find the column with the larget value\n",
    "chosen_classes_per_row = nonscored_targets.iloc[:,1:].idxmax(axis=1)\n",
    "\n",
    "# chosen_classes_per_row.value_counts()\n",
    "\n",
    "# hidden_class_indices = chosen_classes_per_row[chosen_classes_per_row == 'hidden_class'].index\n",
    "\n",
    "# Reference: https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "for index, row in nonscored_targets.iterrows():\n",
    "    max_class = chosen_classes_per_row[index] # Subset the selected class\n",
    "    row[row.index.isin([max_class, 'sig_id']) == False] = 0\n",
    "    nonscored_targets.iloc[index,:] = row\n",
    "\n",
    "# nonscored_targets.to_csv(config.OUTPUT_FILE, index=False)\n",
    "# the hidden_class seems to have dominated the probabilities\n",
    "nonscored_targets.drop(['hidden_class'], axis=1, inplace=True) # drop the hidden_class column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonscored_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y.iloc[:,1:].sum(axis=0)\n",
    "class_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = class_counts.sort_values(ascending=False)\n",
    "class_counts_sub = class_counts.head(12)\n",
    "class_counts_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_classes = class_counts_sub.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the $c=12$ target vectors. For each of the $c$ classes, there should be a binary vector that is '1' if a subject is positive for that class and '0' otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y.iloc[:,1:]\n",
    "y2.head() # create a binary target vector for each class, it should check if there are any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following creates 'c' binary target vectors saved in a list: 'binary_vector_list'\n",
    "class_index_list = [] # Save indices that contain the class\n",
    "for c in retained_classes:\n",
    "    c_indices = y2.loc[:,c][y2.loc[:,c] == 1].index.values\n",
    "    class_index_list.append([c, c_indices])\n",
    "\n",
    "binary_vector_list = []\n",
    "n = len(y)\n",
    "for i in class_index_list: # Loop through class/index pairs\n",
    "    zeros = [0] * n\n",
    "    for j in range(n): # Loop through all rows\n",
    "        # Check if the index should be one instead\n",
    "        if j in i[1]:\n",
    "            zeros[j] = 1\n",
    "    binary_vector_list.append(pd.DataFrame({i[0]: zeros}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(binary_vector_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ovr_output.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_X():\n",
    "    \"\"\"The preprocess_X() function will do the initial preprocessing for the\n",
    "        dataset features.\n",
    "    \"\"\"\n",
    "    X = pd.read_csv(\"../input/train_features.csv\")\n",
    "    X_test = pd.read_csv(\"../input/test_features.csv\")\n",
    "    X.drop(X.columns[0], axis=1, inplace=True)\n",
    "    X_test.drop(X_test.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    # Save the column names\n",
    "    X_col_names = X.columns.tolist()\n",
    "    cat_cols = ['cp_type', 'cp_time', 'cp_dose'] # Identify categorical columns\n",
    "    ohe = OneHotEncoder() # Load OHE\n",
    "    _ = ohe.fit_transform(X[cat_cols])\n",
    "    ohe_names = ohe.get_feature_names(cat_cols)\n",
    "    ohe_names = ohe_names.tolist()\n",
    "\n",
    "    # Fix new column names to include OHE names and normal feature names\n",
    "    X_col_names = [col for col in X_col_names if col\\\n",
    "        not in cat_cols]\n",
    "    ohe_names.extend(X_col_names)\n",
    "\n",
    "    # Transform the data with OHE on the indices of the cat variables\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[('encoder', OneHotEncoder(), list(range(0,3)))],\n",
    "        remainder='passthrough')\n",
    "    X = pd.DataFrame(ct.fit_transform(X))\n",
    "    X.columns = ohe_names\n",
    "    ### Unsure if this is correct ct.transform()\n",
    "    X_test = pd.DataFrame(ct.transform(X_test))\n",
    "    X_test.columns = ohe_names\n",
    "\n",
    "    # Apply feature scaling to the numeric attributes\n",
    "    sc = StandardScaler()\n",
    "    X = X.values\n",
    "    X_test = X_test.values\n",
    "    X[:,7:] = sc.fit_transform(X[:,7:])\n",
    "    X_test[:,7:] = sc.transform(X_test[:,7:])\n",
    "    \n",
    "    return X, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_OVR_targets():\n",
    "    \"\"\"Generate the list of binary OVR target vectors that will be tested.\n",
    "    \"\"\"\n",
    "    y = pd.read_csv(\"../input/train_targets_scored.csv\")\n",
    "    class_counts = y.iloc[:,1:].sum(axis=0)\n",
    "    class_counts = class_counts.sort_values(ascending=False)\n",
    "    \n",
    "    ### Hard coded # of classes\n",
    "\n",
    "    class_counts_sub = class_counts.head(12)\n",
    "    retained_classes = class_counts_sub.index.values\n",
    "    y2 = y.iloc[:,1:]\n",
    "\n",
    "    ### The following creates 'c' binary target vectors saved in a list: 'binary_vector_list'\n",
    "    class_index_list = [] # Save indices that contain the class\n",
    "    for c in retained_classes:\n",
    "        c_indices = y2.loc[:,c][y2.loc[:,c] == 1].index.values\n",
    "        class_index_list.append([c, c_indices])\n",
    "\n",
    "    binary_vector_list = []\n",
    "    n = len(y)\n",
    "    for i in class_index_list: # Loop through class/index pairs\n",
    "        zeros = [0] * n\n",
    "        for j in range(n): # Loop through all rows\n",
    "            # Check if the index should be one instead\n",
    "            if j in i[1]:\n",
    "                zeros[j] = 1\n",
    "        binary_vector_list.append(pd.DataFrame({i[0]: zeros}))\n",
    "\n",
    "    return binary_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_targets = generate_OVR_targets()\n",
    "\n",
    "# Edit to use it for sample submission\n",
    "nonscored_targets = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "nonscored_targets.replace(0.5, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Can OHE and standard scale the X_train first\n",
    "X_train, X_test = preprocess_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ovr_targets:\n",
    "    y_temp = copy.deepcopy(i)\n",
    "    class_name = y_temp.columns[0]\n",
    "\n",
    "    # Intiialize the classifier\n",
    "### Need to later check the correct model for a given feature\n",
    "    model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_temp.values.ravel())\n",
    "\n",
    "    # Create predictions\n",
    "    y_pred_probs = model.predict_proba(X_test)\n",
    "\n",
    "    # Update predicted probabilities\n",
    "    nonscored_targets.loc[:,class_name] = y_pred_probs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through each row and find the column with the larget value\n",
    "chosen_classes_per_row = nonscored_targets.iloc[:,1:].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "for index, row in nonscored_targets.iterrows():\n",
    "    max_class = chosen_classes_per_row[index] # Subset the selected class\n",
    "    row[row.index.isin([max_class, 'sig_id']) == False] = 0\n",
    "    nonscored_targets.iloc[index,:] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonscored_targets.to_csv('../output/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv = pd.read_csv('../output/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end ovr_output.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ovr_cv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first create a simple output so that the log-loss can be developed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CV process\n",
    "    - the preprocessing is tricky here, we need to keep the correct pieces consistent + independent\n",
    "        - We have X_train and y_train, these we will need to split into k-folds.\n",
    "        - The OHE can be done globally. Simple transformations like x1^2 or log() can be done globally.\n",
    "        - Scaling of the data however needs to be done independently, first on the training folds and then applied successively to the validation folds.\n",
    "    - Issues:\n",
    "        - find the stratified k-fold for the multilabel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    \"\"\"The preprocess_X() function will do the initial preprocessing for the\n",
    "        dataset features.\n",
    "    \"\"\"\n",
    "    X = pd.read_csv(\"../input/train_features.csv\")\n",
    "    y = pd.read_csv(\"../input/train_targets_scored.csv\")\n",
    "\n",
    "    # Add hidden class\n",
    "    zero_class_indices = y[y.iloc[:,1:].apply(sum, axis=1) == 0].index\n",
    "    y['hidden_class'] = 0\n",
    "    y.loc[zero_class_indices, 'hidden_class'] = 1\n",
    "#     y['hidden_class'].iloc[zero_class_indices] = 1\n",
    "\n",
    "    class_counts = y.iloc[:,1:].sum(axis=0)\n",
    "    class_counts = class_counts.sort_values(ascending=False)\n",
    "    \n",
    "    ### Hard coded # of classes\n",
    "    \n",
    "    class_counts_sub = class_counts.head(13)\n",
    "    chosen_classes = class_counts_sub.index.values\n",
    "    \n",
    "    X.drop(X.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    # Save the column names\n",
    "    X_col_names = X.columns.tolist()\n",
    "    cat_cols = ['cp_type', 'cp_time', 'cp_dose'] # Identify categorical columns\n",
    "    ohe = OneHotEncoder() # Load OHE\n",
    "    _ = ohe.fit_transform(X[cat_cols])\n",
    "    ohe_names = ohe.get_feature_names(cat_cols)\n",
    "    ohe_names = ohe_names.tolist()\n",
    "\n",
    "    # Fix new column names to include OHE names and normal feature names\n",
    "    X_col_names = [col for col in X_col_names if col\\\n",
    "        not in cat_cols]\n",
    "    ohe_names.extend(X_col_names)\n",
    "\n",
    "    # Transform the data with OHE on the indices of the cat variables\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[('encoder', OneHotEncoder(), list(range(0,3)))],\n",
    "        remainder='passthrough')\n",
    "    X = pd.DataFrame(ct.fit_transform(X))\n",
    "    X.columns = ohe_names\n",
    "\n",
    "    # Reverse the OHE labels\n",
    "    y = y.iloc[:, 1:].idxmax(axis=1)\n",
    "    y = pd.DataFrame(y)\n",
    "    y.columns = [\"target\"]\n",
    "\n",
    "    df = pd.concat([X, y], axis=1)  # Recombine into single df\n",
    "\n",
    "    df[\"kfold\"] = -1  # Create k-folds column\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  # Randomize the dataset\n",
    "\n",
    "    y = df.target.values  # Subset the target column\n",
    "\n",
    "    # Initialize the stratified k-fold module from sklearn\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Fill the 'kfold' column with the assigned folds\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, \"kfold\"] = f\n",
    "\n",
    "    return df, chosen_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, chosen_classes = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_arr_to_df(y_arr):\n",
    "    \"\"\"Change the y array into a y dataframe.\n",
    "    \"\"\"\n",
    "    y_df_template = pd.read_csv('../input/sample_submission.csv')\n",
    "    y_df_template.replace(0.5, 0, inplace=True)\n",
    "    y_df_template['hidden_class'] = 0\n",
    "    y_df_template.drop(['sig_id'], axis=1, inplace=True)\n",
    "\n",
    "    n_rows = y_arr.shape[0]\n",
    "    m_rows = y_df_template.shape[0]\n",
    "    new_rows = n_rows - m_rows\n",
    "    \n",
    "    keys = y_df_template.columns\n",
    "    values = [[0] * new_rows] * len(y_df_template.columns)\n",
    "    extra_rows_dict = dict(zip(keys, values))\n",
    "    extra_rows_df = pd.DataFrame(extra_rows_dict)\n",
    "\n",
    "    y_df_template = y_df_template.append(extra_rows_df, ignore_index=True)\n",
    "\n",
    "    for col_idx in range(len(y_arr)):\n",
    "        y_df_template.loc[col_idx, y_arr[col_idx]] = 1\n",
    "        \n",
    "    return y_df_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_vector_fun(y_df, chosen_classes):\n",
    "    \"\"\"Create a OVR binary vector for a set of chosen classes.\n",
    "    \"\"\"\n",
    "    ### The following creates 'c' binary target vectors saved in a list: 'binary_vector_list'\n",
    "    class_index_list = [] # Save indices that contain the class\n",
    "    for c in chosen_classes:\n",
    "        c_indices = y_df.loc[:,c][y_df.loc[:,c] == 1].index.values\n",
    "        class_index_list.append([c, c_indices])\n",
    "\n",
    "    binary_vector_list = []\n",
    "    n = y_df.shape[0]\n",
    "    for i in class_index_list: # Loop through class/index pairs\n",
    "        zeros = [0] * n\n",
    "        for j in range(n): # Loop through all rows\n",
    "            # Check if the index should be one instead\n",
    "            if j in i[1]:\n",
    "                zeros[j] = 1\n",
    "        binary_vector_list.append(pd.DataFrame({i[0]: zeros}))\n",
    "\n",
    "    return binary_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_log_loss(y_valid, y_pred):\n",
    "    \"\"\"Calculate the log-loss for the multilabel case.\n",
    "    \"\"\"\n",
    "    N, M = y_valid.shape # Create temp matrix to store values\n",
    "    zero_mat = np.zeros((N,M))\n",
    "\n",
    "    dummy_zero = 1*10**(-15) # Compensate for 0's and 1's predictions\n",
    "    y_pred.replace(0, dummy_zero, inplace=True)\n",
    "    y_pred.replace(1, 1-dummy_zero, inplace=True)\n",
    "\n",
    "    for m in range(M): # Calculate log-loss per index\n",
    "        for n in range(N):\n",
    "            y_true = y_valid.iloc[n,m]\n",
    "            y_hat = y_pred.iloc[n,m]\n",
    "            temp_log_loss = y_true * np.log(y_hat) +\\\n",
    "                (1 - y_true) * np.log(1 - y_hat)            \n",
    "            zero_mat[n,m] = temp_log_loss\n",
    "\n",
    "    log_loss_score = -zero_mat.mean(axis=0).mean()\n",
    "    \n",
    "    return log_loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(fold, chosen_classes=chosen_classes):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    # Drop the target and and convert to numpy\n",
    "    x_train = df_train.drop([\"target\", \"kfold\"], axis=1).values\n",
    "    y_train = df_train.target.values\n",
    "    y_train_df = y_arr_to_df(y_arr=y_train)\n",
    "\n",
    "    # Do OVR encoding on the training targets\n",
    "    ovr_targets = binary_vector_fun(y_df=y_train_df, chosen_classes=chosen_classes)\n",
    "\n",
    "    # Repeat for validation data\n",
    "    x_valid = df_valid.drop([\"target\", \"kfold\"], axis=1).values\n",
    "    y_valid = df_valid.target.values\n",
    "    y_valid = y_arr_to_df(y_arr=y_valid)\n",
    "\n",
    "    # Apply feature scaling to the numeric attributes\n",
    "    sc = StandardScaler()\n",
    "    x_train[:, 7:] = sc.fit_transform(x_train[:, 7:])\n",
    "    x_valid[:, 7:] = sc.transform(x_valid[:, 7:])\n",
    "\n",
    "    ### Need a non-scored df of dimensions equal to validation set\n",
    "    non_scored_y_valid = copy.deepcopy(y_valid)\n",
    "    non_scored_y_valid.replace(1, 0, inplace=True)\n",
    "\n",
    "    ### So interestingly, this needs to be repeated for each of the chosen classes.\n",
    "    for i in ovr_targets:\n",
    "        y_temp = copy.deepcopy(i)\n",
    "        class_name = y_temp.columns[0]\n",
    "\n",
    "        # Intiialize the classifier\n",
    "        model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(x_train, y_temp.values.ravel())\n",
    "\n",
    "        # Create predictions\n",
    "        y_pred_probs = model.predict_proba(x_valid)\n",
    "\n",
    "        # Fill the non-scored y's\n",
    "        non_scored_y_valid.loc[:,class_name] = y_pred_probs[:,1]\n",
    "\n",
    "    # Go through each row and find the column with the larget value\n",
    "    chosen_classes_per_row = non_scored_y_valid.iloc[:,1:].idxmax(axis=1)\n",
    "\n",
    "    for index, row in non_scored_y_valid.iterrows():\n",
    "        max_class = chosen_classes_per_row[index] # Subset the selected class\n",
    "        row[row.index.isin([max_class, 'sig_id']) == False] = 0\n",
    "        non_scored_y_valid.iloc[index,:] = row\n",
    "\n",
    "    # drop the hidden_class column\n",
    "    non_scored_y_valid.drop(['hidden_class'], axis=1, inplace=True)\n",
    "    y_valid.drop(['hidden_class'], axis=1, inplace=True)\n",
    "\n",
    "    log_loss_score = multilabel_log_loss(y_valid=y_valid, y_pred=non_scored_y_valid)\n",
    "\n",
    "    print(f\"Fold={fold}, Log-Loss={log_loss_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_ in range(5):\n",
    "    run_cv(fold_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ovr_cv.py end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterstrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    \"\"\"Preprocess the data.\n",
    "    \"\"\"\n",
    "    X = pd.read_csv(\"../input/train_features.csv\")\n",
    "    X.drop(X.columns[0], axis=1, inplace=True)\n",
    "    y = pd.read_csv(\"../input/train_targets_scored.csv\")\n",
    "    y.drop(y.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    # Add hidden class\n",
    "    zero_class_indices = y[y.iloc[:,1:].apply(sum, axis=1) == 0].index\n",
    "    y['hidden_class'] = 0\n",
    "    y.loc[zero_class_indices, 'hidden_class'] = 1\n",
    "    \n",
    "    class_counts = y.iloc[:,1:].sum(axis=0)\n",
    "    class_counts = class_counts.sort_values(ascending=False)\n",
    "    class_counts_sub = class_counts.head(13)\n",
    "    chosen_classes = class_counts_sub.index.values\n",
    "\n",
    "    # Save the column names\n",
    "    X_col_names = X.columns.tolist()\n",
    "    cat_cols = ['cp_type', 'cp_time', 'cp_dose'] # Identify categorical columns\n",
    "    ohe = OneHotEncoder() # Load OHE\n",
    "    _ = ohe.fit_transform(X[cat_cols])\n",
    "    ohe_names = ohe.get_feature_names(cat_cols)\n",
    "    ohe_names = ohe_names.tolist()\n",
    "\n",
    "    # Fix new column names to include OHE names and normal feature names\n",
    "    X_col_names = [col for col in X_col_names if col\\\n",
    "        not in cat_cols]\n",
    "    ohe_names.extend(X_col_names)\n",
    "\n",
    "    # Transform the data with OHE on the indices of the cat variables\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[('encoder', OneHotEncoder(), list(range(0,3)))],\n",
    "        remainder='passthrough')\n",
    "    X = pd.DataFrame(ct.fit_transform(X))\n",
    "    X.columns = ohe_names\n",
    "\n",
    "    train_idx_list = []; valid_idx_list = []\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_index, valid_index in mskf.split(X, y):\n",
    "        train_idx_list.append(train_index)\n",
    "        valid_idx_list.append(valid_index)\n",
    "        \n",
    "    return X, y, train_idx_list, valid_idx_list, chosen_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, train_idx_list, valid_idx_list, chosen_classes = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_msfk_fun(y_df, chosen_classes):\n",
    "    \"\"\"Create a OVR binary vector for a set of chosen classes.\n",
    "    \"\"\"\n",
    "    y_df_copy=copy.deepcopy(y_df)\n",
    "    y_df_copy.reset_index(drop=True, inplace=True)\n",
    "    chosen_classes=chosen_classes\n",
    "\n",
    "    ### The following creates 'c' binary target vectors saved in a list: 'binary_vector_list'\n",
    "    class_index_list = [] # Save indices that contain the class\n",
    "    for c in chosen_classes:\n",
    "        # These are row-indices\n",
    "        c_indices = y_df_copy[y_df_copy.loc[:,c] == 1].loc[:,c]\n",
    "        class_index_list.append([c, c_indices])\n",
    "\n",
    "    # For each class, generate a binary target vector\n",
    "    binary_vector_list = []; n = y_df.shape[0]\n",
    "\n",
    "    for i in class_index_list: # Loop through class/index pairs\n",
    "        zeros = [0] * n # Can't do this actually\n",
    "        for j in range(n): # Loop through all rows\n",
    "            # Check if the index should be one instead\n",
    "            if j in i[1]:\n",
    "                zeros[j] = 1\n",
    "        binary_vector_list.append(pd.DataFrame({i[0]: zeros}))\n",
    "\n",
    "    return binary_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(fold, X, y, train_idx_list, valid_idx_list, chosen_classes, log_loss_list):\n",
    "    \"\"\"Run the cross-validation.\"\"\"\n",
    "    train_idx = train_idx_list[fold]; valid_idx = valid_idx_list[fold]\n",
    "\n",
    "    ### These have shifted row names\n",
    "    x_train = X.iloc[train_idx,:].values; y_train = y.iloc[train_idx,:]\n",
    "    x_valid = X.iloc[valid_idx,:].values; y_valid = y.iloc[valid_idx,:]\n",
    "\n",
    "    # Apply feature scaling to the numeric attributes\n",
    "    sc = StandardScaler()\n",
    "    x_train[:, 7:] = sc.fit_transform(x_train[:, 7:])\n",
    "    x_valid[:, 7:] = sc.transform(x_valid[:, 7:])\n",
    "\n",
    "    ### Need a non-scored df of dimensions equal to validation set\n",
    "    non_scored_y_valid = copy.deepcopy(y_valid)\n",
    "    non_scored_y_valid.replace(1, 0, inplace=True)\n",
    "\n",
    "    # Do OVR encoding on the training targets\n",
    "    ovr_targets = binary_msfk_fun(y_df=y_train, chosen_classes=chosen_classes)\n",
    "\n",
    "    for i in ovr_targets: # Loop through the OVR targets and fit a model\n",
    "        y_temp = copy.deepcopy(i)\n",
    "        class_name = y_temp.columns[0]\n",
    "\n",
    "        # Intiialize the classifier\n",
    "        model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "    ### Is this a bug? There seems to be a class name included in the fit...\n",
    "        # Fit the model\n",
    "        model.fit(x_train, y_temp.values.ravel())\n",
    "\n",
    "        # Create predictions\n",
    "        y_pred_probs = model.predict_proba(x_valid)\n",
    "\n",
    "        # Fill the non-scored y's\n",
    "        non_scored_y_valid.loc[:, class_name] = y_pred_probs[:, 1]\n",
    "\n",
    "    # Go through each row and find the column with the larget value\n",
    "    chosen_classes_per_row = non_scored_y_valid.iloc[:, 1:].idxmax(axis=1)\n",
    "\n",
    "    for index, row in non_scored_y_valid.iterrows():\n",
    "        max_class = chosen_classes_per_row[index]  # Subset the selected class\n",
    "        row[row.index.isin([max_class]) == False] = 0\n",
    "        non_scored_y_valid.loc[index, :] = row\n",
    "\n",
    "    # drop the hidden_class column\n",
    "    non_scored_y_valid.drop([\"hidden_class\"], axis=1, inplace=True)\n",
    "    y_valid = y_valid.drop([\"hidden_class\"], axis=1)\n",
    "\n",
    "    log_loss_score = multilabel_log_loss(y_valid=y_valid, y_pred=non_scored_y_valid)\n",
    "\n",
    "    print(f\"Fold={fold}, Log-Loss={log_loss_score}\")\n",
    "    log_loss_list.append(log_loss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_list = []\n",
    "for fold_ in range(5):\n",
    "    run_cv(fold_, X, y, train_idx_list, valid_idx_list, chosen_classes, log_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_list = []; test_idx_list = []\n",
    "for train_index, test_index in mskf.split(X, y):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "#     y_train, y_test = y.iloc[train_index,:], y.iloc[test_index,:]\n",
    "    train_idx_list.append(train_index)\n",
    "    test_idx_list.append(test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end iterstrat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cf_param_search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree, kernelsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_list = ['log_reg', 'svm', 'rf', 'knn', 'nb', 'xgb']\n",
    "clf_list = ['log_reg', 'svm', 'rf', 'knn', 'nb']\n",
    "\n",
    "param_grid = {\n",
    "    'log_reg' : {\n",
    "        'Penalty': ['l2'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [round((0.1) * ((0.1) ** (n - 1)), 5) for n in reversed(range(-3,4))],\n",
    "        'gamma': ['auto'],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'probability': [True]\n",
    "    },\n",
    "    'rf': {\n",
    "        'n_estimators': [120, 300, 500, 800, 1200],\n",
    "        'max_depth': [5, 8, 15, 25, 30, None],\n",
    "        'min_samples_split': [1, 2, 5, 10, 15, 100],\n",
    "        'min_samples_leaf': [1, 2, 5, 10],\n",
    "        'max_features': ['log2', 'sqrt', None]\n",
    "    },\n",
    "    'df': {\n",
    "        'max_depth': [5, 8, 15, 25, 30, None],\n",
    "        'min_samples_split': [1, 2, 5, 10, 15, 100],\n",
    "        'min_samples_leaf': [1, 2, 5, 10],\n",
    "        'max_features': ['log2', 'sqrt', None]        \n",
    "    },\n",
    "    'knn': {\n",
    "        'n_neighbors': [round((2) * ((2) ** (n - 1)), 5) for n in range(1, 7)],\n",
    "        'p': [2, 3]\n",
    "    },\n",
    "    'nb': {\n",
    "        'dummy_param': [None, None]\n",
    "    },\n",
    "    'xgb': { # Reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "        'eta': [0.01, 0.015, 0.025, 0.05, 0.1],\n",
    "        'gamma': [0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "        'max_depth': [3, 5, 7, 9, 12, 15, 17, 25],\n",
    "        'min_child_weight': [1, 3, 5, 7],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'lambda': [0.01, 0.1, 1.0],\n",
    "        'alpha': [0, 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_params(clf_name, params):\n",
    "    \"\"\"Set the parameters for a model during grid search.\n",
    "    \"\"\"\n",
    "    if clf_name == 'log_reg':    \n",
    "        model = LogisticRegression(\n",
    "            penalty=params[0],\n",
    "            C=params[1],\n",
    "            random_state=0,\n",
    "            max_iter=1e10,\n",
    "        )\n",
    "    elif clf_name == 'svm':\n",
    "        model = SVC(\n",
    "            C=params[0],\n",
    "            gamma=params[1],\n",
    "            class_weight=params[2],\n",
    "            probability=params[3]\n",
    "        )\n",
    "    elif clf_name == 'rf':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=params[0],\n",
    "            max_depth=params[1],\n",
    "            min_samples_split=params[2],\n",
    "            min_samples_leaf=params[3],\n",
    "            max_features=params[4]\n",
    "        )\n",
    "    elif clf_name == 'dt':\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=params[0],\n",
    "            min_samples_split=params[1],\n",
    "            min_samples_leaf=params[2],\n",
    "            max_features=params[3]\n",
    "        )\n",
    "    elif clf_name == 'knn':\n",
    "        model = KNeighborsClassifier(\n",
    "            n_neighbors=params[0],\n",
    "            p=params[1]\n",
    "        )\n",
    "    elif clf_name == 'nb':\n",
    "        model = GaussianNB()\n",
    "    elif clf_name == 'xgb':\n",
    "        model = XGBClassifier(\n",
    "            learning_rate=params[0],\n",
    "            gamma=params[1],\n",
    "            max_depth=params[2],\n",
    "            min_child_weight=params[3],\n",
    "            subsample=params[4],\n",
    "            colsample_bytree=params[5],\n",
    "            reg_lambda=params[6],\n",
    "            reg_alpha=params[7]\n",
    "        )\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_idx = clf_list[1]\n",
    "clf_param_grid = param_grid[clf_idx]\n",
    "param_names = sorted(clf_param_grid) # Create parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_idx in clf_list: # Loop through models\n",
    "    clf_param_grid = param_grid[clf_idx]\n",
    "\n",
    "    param_names = [key for key in clf_param_grid.keys()] # Create parameter combinations\n",
    "    param_combos = itertools.product(*(clf_param_grid[p_name] for p_name in param_names))\n",
    "    param_combos_list = list(param_combos)\n",
    "\n",
    "    for p_combo_idx in range(len(param_combos_list)): # Loop through parameters\n",
    "        # Intiialize the classifier\n",
    "        param_combo_ = param_combos_list[p_combo_idx]\n",
    "#         print(param_combo_)\n",
    "        model_ = set_model_params(clf_name=clf_idx, params=param_combo_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_idx = clf_list[0]\n",
    "clf_param_grid = param_grid[clf_idx]\n",
    "clf_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_name = 'log_reg'\n",
    "params = param_combos_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_idx in clf_list: # Loop through models\n",
    "    clf_param_grid = param_grid[clf_idx]\n",
    "    \n",
    "    param_names = sorted(clf_param_grid) # Create parameter combinations\n",
    "    param_combos = itertools.product(*(clf_param_grid[p_name] for p_name in param_names))\n",
    "    param_combos_list = list(param_combos)\n",
    "    \n",
    "    for p_combo_idx in range(len(param_combos_list)): # Loop through model parameters\n",
    "        temp_param_combo = param_combos_list[p_combo_idx]  # (C, Penalty)\n",
    "        model = set_model_params(clf_name=clf_idx, params=temp_param_combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = parameters['log_reg']\n",
    "param_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = sorted(param_test)\n",
    "param_combos = itertools.product(*(param_test[p_name] for p_name in param_names))\n",
    "param_combos_list = list(param_combos)\n",
    "param_combos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    penalty=temp_param_combo[1],\n",
    "    C=temp_param_combo[0],\n",
    "    random_state=0,\n",
    "    max_iter=1e10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "nb = GaussianNB()\n",
    "svm = SVC()\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('logreg', log_reg),\n",
    "#     ('nb', nb),\n",
    "#     ('svm', svm)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end cf_param_search.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_selection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently:\n",
    "- loop through models\n",
    "    - loop through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end model_selection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Test a model\n",
    "<a id=\"test1\"></a>\n",
    "<a href=\"#top\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df_template[y_arr[0]].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../input/train_features.csv')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test add a new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature = list(range(X.shape[0]))\n",
    "X['new_feature'] = new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end test to add feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = binary_vector_list[0]\n",
    "y3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(X.columns[0], axis=1, inplace=True)\n",
    "# Save the column names\n",
    "X_col_names = X.columns.tolist()\n",
    "\n",
    "cat_cols = ['cp_type', 'cp_time', 'cp_dose'] # Identify categorical columns\n",
    "\n",
    "ohe = OneHotEncoder() # Load OHE\n",
    "\n",
    "# Get the column names after OHE\n",
    "# Reference: https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder\n",
    "_ = ohe.fit_transform(X[cat_cols])\n",
    "ohe_names = ohe.get_feature_names(cat_cols)\n",
    "ohe_names = ohe_names.tolist()\n",
    "\n",
    "# Fix new column names to include OHE names and normal feature names\n",
    "X_col_names = [col for col in X_col_names if col\\\n",
    "    not in cat_cols]\n",
    "ohe_names.extend(X_col_names)\n",
    "\n",
    "# Transform the data with OHE on the indices of the cat variables\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('encoder', OneHotEncoder(), list(range(0,3)))],\n",
    "    remainder='passthrough')\n",
    "X = pd.DataFrame(ct.fit_transform(X))\n",
    "X.columns = ohe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3.columns = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X, y3], axis=1) # Recombine into single df\n",
    "\n",
    "df['kfold'] = -1 # Create k-folds column\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True) # Randomize the dataset\n",
    "\n",
    "y = df.target.values # Subset the target column\n",
    "\n",
    "# Initialize the stratified k-fold module from sklearn\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Fill the 'kfold' column with the assigned folds\n",
    "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    # Drop the target and and convert to numpy\n",
    "    x_train = df_train.drop(['target', 'kfold'], axis=1).values\n",
    "    y_train = df_train.target.values\n",
    "\n",
    "    # Repeat for validation data\n",
    "    x_valid = df_valid.drop(['target', 'kfold'], axis=1).values\n",
    "    y_valid = df_valid.target.values\n",
    "\n",
    "    # Apply feature scaling to the numeric attributes\n",
    "    sc = StandardScaler()\n",
    "    x_train[:,7:] = sc.fit_transform(x_train[:,7:])\n",
    "    x_valid[:,7:] = sc.transform(x_valid[:,7:])\n",
    "\n",
    "    # Intiialize the classifier\n",
    "    model = linear_model.LogisticRegression(random_state=0, max_iter=1e10)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Create predictions\n",
    "    y_pred_probs = model.predict_proba(x_valid)\n",
    "    y_preds = model.predict(x_valid)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    log_loss_score = metrics.log_loss(y_valid, y_pred_probs,\n",
    "        labels=model.classes_)\n",
    "    auc = metrics.roc_auc_score(y_valid, y_preds,\n",
    "        labels=model.classes_)\n",
    "    accuracy = metrics.accuracy_score(y_valid, y_preds)\n",
    "    \n",
    "    print(f\"Fold={fold}, Log-Loss={log_loss_score}, AUC={auc}, Accuracy={accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do CV for all the $c$ classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in binary_vector_list:\n",
    "    y_temp = copy.deepcopy(i)\n",
    "    class_name = y_temp.columns[0]\n",
    "    X = pd.read_csv('../input/train_features.csv')\n",
    "    X.drop(X.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    # Save the column names\n",
    "    X_col_names = X.columns.tolist()\n",
    "\n",
    "    cat_cols = ['cp_type', 'cp_time', 'cp_dose'] # Identify categorical columns\n",
    "\n",
    "    ohe = OneHotEncoder() # Load OHE\n",
    "\n",
    "    # Get the column names after OHE\n",
    "    # Reference: https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder\n",
    "    _ = ohe.fit_transform(X[cat_cols])\n",
    "    ohe_names = ohe.get_feature_names(cat_cols)\n",
    "    ohe_names = ohe_names.tolist()\n",
    "\n",
    "    # Fix new column names to include OHE names and normal feature names\n",
    "    X_col_names = [col for col in X_col_names if col\\\n",
    "        not in cat_cols]\n",
    "    ohe_names.extend(X_col_names)\n",
    "\n",
    "    # Transform the data with OHE on the indices of the cat variables\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[('encoder', OneHotEncoder(), list(range(0,3)))],\n",
    "        remainder='passthrough')\n",
    "    X = pd.DataFrame(ct.fit_transform(X))\n",
    "    X.columns = ohe_names\n",
    "    \n",
    "    y_temp.columns = ['target']\n",
    "    \n",
    "    df = pd.concat([X, y_temp], axis=1) # Recombine into single df\n",
    "\n",
    "    df['kfold'] = -1 # Create k-folds column\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True) # Randomize the dataset\n",
    "\n",
    "    y = df.target.values # Subset the target column\n",
    "\n",
    "    # Initialize the stratified k-fold module from sklearn\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Fill the 'kfold' column with the assigned folds\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "        \n",
    "    print(class_name)\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple goal: Try to create predictions for the chosen classes, and output 0's for the others. Make it scalable, so that the chosen classes can be altered.\n",
    "\n",
    "### Need: Output the probabilities so that the log-loss can be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_targets = pd.read_csv('../input/train_targets_nonscored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = class_counts.sort_values(ascending=False)\n",
    "class_counts\n",
    "# class_counts_sub = class_counts.head(12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

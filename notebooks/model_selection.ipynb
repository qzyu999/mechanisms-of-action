{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import itertools\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "pd.set_option('display.max_rows', None) # Show max rows/columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_log_loss(y_valid, y_pred):\n",
    "    \"\"\"Calculate the log-loss for the multilabel case.\"\"\"\n",
    "    N, M = y_valid.shape  # Create temp matrix to store values\n",
    "    zero_mat = np.zeros((N, M))\n",
    "\n",
    "    dummy_zero = 1 * 10 ** (-15)  # Compensate for 0's and 1's predictions\n",
    "    y_pred.replace(0, dummy_zero, inplace=True)\n",
    "    y_pred.replace(1, 1 - dummy_zero, inplace=True)\n",
    "\n",
    "    for m in range(M):  # Calculate log-loss per index\n",
    "        for n in range(N):\n",
    "            y_true = y_valid.iloc[n, m]\n",
    "            y_hat = y_pred.iloc[n, m]\n",
    "            temp_log_loss = y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat)\n",
    "            zero_mat[n, m] = temp_log_loss\n",
    "\n",
    "    log_loss_score = -zero_mat.mean(axis=0).mean()\n",
    "\n",
    "    return log_loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_FILE=\"../input/train_features.csv\"\n",
    "TARGETS_FILE=\"../input/train_targets_scored.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    \"\"\"Preprocess the data.\"\"\"\n",
    "    X = pd.read_csv(FEATURES_FILE)\n",
    "    X.drop(X.columns[0], axis=1, inplace=True)\n",
    "    y = pd.read_csv(TARGETS_FILE)\n",
    "    y.drop(y.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    # Add hidden class\n",
    "    zero_class_indices = y[y.iloc[:, 1:].apply(sum, axis=1) == 0].index\n",
    "    y[\"hidden_class\"] = 0\n",
    "    y.loc[zero_class_indices, \"hidden_class\"] = 1\n",
    "\n",
    "    class_counts = y.iloc[:, 1:].sum(axis=0)\n",
    "    class_counts = class_counts.sort_values(ascending=False)\n",
    "    class_counts_sub = class_counts.head(13)\n",
    "    chosen_classes = class_counts_sub.index.values\n",
    "\n",
    "    # Save the column names\n",
    "    X_col_names = X.columns.tolist()\n",
    "    cat_cols = [\"cp_type\", \"cp_time\", \"cp_dose\"]  # Identify categorical columns\n",
    "    ohe = OneHotEncoder()  # Load OHE\n",
    "    _ = ohe.fit_transform(X[cat_cols])\n",
    "    ohe_names = ohe.get_feature_names(cat_cols)\n",
    "    ohe_names = ohe_names.tolist()\n",
    "\n",
    "    # Fix new column names to include OHE names and normal feature names\n",
    "    X_col_names = [col for col in X_col_names if col not in cat_cols]\n",
    "    ohe_names.extend(X_col_names)\n",
    "\n",
    "    # Transform the data with OHE on the indices of the cat variables\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[(\"encoder\", OneHotEncoder(), list(range(0, 3)))],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    X = pd.DataFrame(ct.fit_transform(X))\n",
    "    X.columns = ohe_names\n",
    "\n",
    "    train_idx_list = []\n",
    "    valid_idx_list = []\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_index, valid_index in mskf.split(X, y):\n",
    "        train_idx_list.append(train_index)\n",
    "        valid_idx_list.append(valid_index)\n",
    "\n",
    "    return X, y, train_idx_list, valid_idx_list, chosen_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_params(clf_name, params):\n",
    "    \"\"\"Set the parameters for a model during grid search.\"\"\"\n",
    "    if clf_name == \"log_reg\":\n",
    "        model = LogisticRegression(\n",
    "            penalty=params[0],\n",
    "            C=params[1],\n",
    "            random_state=0,\n",
    "            max_iter=1e10,\n",
    "        )\n",
    "    elif clf_name == \"svm\":\n",
    "        model = SVC(\n",
    "            C=params[0], gamma=params[1], class_weight=params[2], probability=params[3]\n",
    "        )\n",
    "    elif clf_name == \"rf\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=params[0],\n",
    "            max_depth=params[1],\n",
    "            min_samples_split=params[2],\n",
    "            min_samples_leaf=params[3],\n",
    "            max_features=params[4],\n",
    "        )\n",
    "    elif clf_name == \"dt\":\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=params[0],\n",
    "            min_samples_split=params[1],\n",
    "            min_samples_leaf=params[2],\n",
    "            max_features=params[3],\n",
    "        )\n",
    "    elif clf_name == \"knn\":\n",
    "        model = KNeighborsClassifier(n_neighbors=params[0], p=params[1])\n",
    "    elif clf_name == \"nb\":\n",
    "        model = GaussianNB()\n",
    "#     elif clf_name == 'xgb':\n",
    "#         model = XGBClassifier(\n",
    "#             learning_rate=params[0],\n",
    "#             gamma=params[1],\n",
    "#             max_depth=params[2],\n",
    "#             min_child_weight=params[3],\n",
    "#             subsample=params[4],\n",
    "#             colsample_bytree=params[5],\n",
    "#             reg_lambda=params[6],\n",
    "#             reg_alpha=params[7]\n",
    "#         )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [\"log_reg\", \"svm\", \"rf\", \"dt\", \"knn\", \"nb\"]\n",
    "\n",
    "param_grid = {\n",
    "    \"log_reg\": {\n",
    "        \"Penalty\": [\"l2\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1],\n",
    "    },\n",
    "    \"svm\": {\n",
    "        \"C\": [round((0.1) * ((0.1) ** (n - 1)), 5) for n in reversed(range(-1, 3))],\n",
    "        \"gamma\": [\"auto\"],\n",
    "        \"class_weight\": [\"balanced\"],\n",
    "        \"probability\": [True],\n",
    "    },\n",
    "    \"rf\": {\n",
    "        \"n_estimators\": [120],\n",
    "        \"max_depth\": [5],\n",
    "        \"min_samples_split\": [5],\n",
    "        \"min_samples_leaf\": [1, 10],\n",
    "        \"max_features\": [\"log2\"],\n",
    "    },\n",
    "    \"dt\": {\n",
    "        \"max_depth\": [5, 25],\n",
    "        \"min_samples_split\": [2, 10],\n",
    "        \"min_samples_leaf\": [2, 10],\n",
    "        \"max_features\": [\"log2\"],\n",
    "    },\n",
    "    \"knn\": {\n",
    "        \"n_neighbors\": [round((2) * ((2) ** (n - 1)), 5) for n in range(1, 3)],\n",
    "        \"p\": [2, 3],\n",
    "    },\n",
    "    \"nb\": {\n",
    "        \"dummy_param\": [None, None]\n",
    "    },\n",
    "    'xgb': {\n",
    "        'eta': [0.01, 0.015, 0.025, 0.05, 0.1],\n",
    "        'gamma': [0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "        'max_depth': [3, 5, 7, 9, 12, 15, 17, 25],\n",
    "        'min_child_weight': [1, 3, 5, 7],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'lambda': [0.01, 0.1, 1.0],\n",
    "        'alpha': [0, 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently:\n",
    "- loop through models\n",
    "    - loop through parameters\n",
    "        - do CV\n",
    "        \n",
    "Ideally:\n",
    "- loop through $K$ classes\n",
    "    - loop through models\n",
    "        - loop through parameters\n",
    "            - do CV\n",
    "- find the best model+parameters per class\n",
    "    - within each $k$'th class, decide which model+parameter combination has the best \"log-loss\"\n",
    "        - for each model-param combo, calculate the avg. CV log-loss\n",
    "            - output/print the model-param combo that performs best\n",
    "            \n",
    "Convenient to current coding:\n",
    "- loop through models\n",
    "    - loop through parameters\n",
    "        - Rather than do regular CV, we can maybe save the output of the log-loss per binary class to some list. This can be averaged with CV.\n",
    "        - e.g., for fold=0, do the train/valid split, then for each of the $K$ classes save the resulting fold 0's log-loss, repeat for all folds, then in the end average over the folds per $K$ class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_df=y_train\n",
    "# chosen_class=chosen_class\n",
    "def binary_msfk_fun(y_df, chosen_class):\n",
    "    \"\"\"Create a OVR binary vector for a set ofchosen classes.\"\"\"\n",
    "    y_df_copy = copy.deepcopy(y_df) # Create copy of target df\n",
    "    y_df_copy.reset_index(drop=True, inplace=True) # Reset row indices\n",
    "    c_indices = y_df_copy[y_df_copy.loc[:, chosen_class] == 1].loc[:, chosen_class]\n",
    "    n = y_df.shape[0]\n",
    "    zeros = [0] * n  # Can't do this actually\n",
    "\n",
    "    for j in range(n):  # Loop through all rows\n",
    "        # Check if the index should be one instead\n",
    "        if j in c_indices:\n",
    "            zeros[j] = 1\n",
    "\n",
    "    binary_target = pd.DataFrame({chosen_class: zeros})\n",
    "    \n",
    "    return binary_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_wise_log_loss(y_true, y_hat):\n",
    "    \"\"\"Calculate the log-loss for just a chosen class.\"\"\"\n",
    "    class_log_loss = y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat)\n",
    "    class_log_loss = -np.mean(class_log_loss)\n",
    "    \n",
    "    return class_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(\n",
    "    fold,\n",
    "    X,\n",
    "    y,\n",
    "    train_idx_list,\n",
    "    valid_idx_list,\n",
    "    chosen_class,\n",
    "    fold_log_loss_list,\n",
    "    param_combo_,\n",
    "    model,\n",
    "):\n",
    "    \"\"\"Run the cross-validation.\"\"\"\n",
    "    train_idx = train_idx_list[fold]\n",
    "    valid_idx = valid_idx_list[fold]\n",
    "\n",
    "    ### These have shifted row names\n",
    "    x_train = X.iloc[train_idx, :].values\n",
    "    y_train = y.iloc[train_idx, :]\n",
    "    x_valid = X.iloc[valid_idx, :].values\n",
    "    y_valid = y.iloc[valid_idx, :]\n",
    "\n",
    "    # Apply feature scaling to the numeric attributes\n",
    "    sc = StandardScaler()\n",
    "    x_train[:, 7:] = sc.fit_transform(x_train[:, 7:])\n",
    "    x_valid[:, 7:] = sc.transform(x_valid[:, 7:])\n",
    "\n",
    "    ### Need a non-scored df of dimensions equal to validation set\n",
    "    non_scored_y_valid = copy.deepcopy(y_valid)\n",
    "    non_scored_y_valid.replace(1, 0, inplace=True)\n",
    "\n",
    "    y_temp = binary_msfk_fun(y_df=y_train, chosen_class=chosen_class)\n",
    "\n",
    "    class_name = y_temp.columns[0]\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_temp.values.ravel())\n",
    "\n",
    "    # Create predictions\n",
    "    y_pred_probs = model.predict_proba(x_valid)[:, 1]\n",
    "\n",
    "    # Calculate the class-log-loss and save it to the list\n",
    "    class_log_loss_score = class_wise_log_loss(y_true=y_valid.loc[:,class_name], y_hat=y_pred_probs)\n",
    "    fold_log_loss_list.append(class_log_loss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaredyu/Desktop/projects/learn_pytorch/env/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Note: This takes a few seconds\n",
    "X, y, train_idx_list, valid_idx_list, chosen_classes = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_classes = chosen_classes[0:3] # temp limit the chosen classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to store all the results?\n",
    "- list of lists of lists:\n",
    "    - outer list: $K$ classes\n",
    "    - center list: $C$ classifiers\n",
    "    - inner list: $P$ parameter combinations\n",
    "    \n",
    "- dictionary is more coherent, dict of dict of dict:\n",
    "    - outer dict: $K$ classes\n",
    "    - center dict: $C$ classifiers\n",
    "    - inner dict: $P$ parameter combinations\n",
    "    \n",
    "- perhaps better is a `dict[class][clf][list]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This way just stored a dict[class][clf][list]\n",
    "# it is less concise, but simpler to make for now\n",
    "result_dict = {}\n",
    "for class_ in chosen_classes:\n",
    "    temp_clf_dict = {}\n",
    "    for clf_ in clf_list:\n",
    "        temp_clf_dict[clf_] = []\n",
    "    \n",
    "    result_dict[class_] = temp_clf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_list = [\"log_reg\", \"rf\"]\n",
    "clf_list = [\"dt\", \"knn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- log_reg is fast, can try all params possibly\n",
    "- rf works ok, can try a few params\n",
    "- dt is fast, can try all params possibly\n",
    "- knn works, but it is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hidden_class', 'nfkb_inhibitor', 'proteasome_inhibitor',\n",
       "       'cyclooxygenase_inhibitor', 'dopamine_receptor_antagonist',\n",
       "       'serotonin_receptor_antagonist', 'dna_inhibitor',\n",
       "       'glutamate_receptor_antagonist', 'adrenergic_receptor_antagonist',\n",
       "       'cdk_inhibitor', 'egfr_inhibitor', 'tubulin_inhibitor',\n",
       "       'acetylcholine_receptor_antagonist'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = chosen_classes[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_classes_list = chosen_classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_classes_list.index(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: hidden_class, index 1 out of 13\n",
      "Class: nfkb_inhibitor, index 2 out of 13\n",
      "Class: proteasome_inhibitor, index 3 out of 13\n",
      "Class: cyclooxygenase_inhibitor, index 4 out of 13\n",
      "Class: dopamine_receptor_antagonist, index 5 out of 13\n",
      "Class: serotonin_receptor_antagonist, index 6 out of 13\n",
      "Class: dna_inhibitor, index 7 out of 13\n",
      "Class: glutamate_receptor_antagonist, index 8 out of 13\n",
      "Class: adrenergic_receptor_antagonist, index 9 out of 13\n",
      "Class: cdk_inhibitor, index 10 out of 13\n",
      "Class: egfr_inhibitor, index 11 out of 13\n",
      "Class: tubulin_inhibitor, index 12 out of 13\n",
      "Class: acetylcholine_receptor_antagonist, index 13 out of 13\n"
     ]
    }
   ],
   "source": [
    "chosen_classes_list = chosen_classes.tolist()\n",
    "for class_idx in chosen_classes: # Loop through classes\n",
    "    ith_class = chosen_classes_list.index(class_idx)\n",
    "    print(f\"Class: {class_idx}, index {ith_class + 1} out of {len(chosen_classes_list)}\")\n",
    "    for clf_idx in clf_list:  # Loop through models\n",
    "        print(f\"Classifier: {clf_idx}\")\n",
    "        \n",
    "        # Create parameter combinations\n",
    "        clf_param_grid = param_grid[clf_idx]\n",
    "        param_names = [\n",
    "            key for key in clf_param_grid.keys()\n",
    "        ]\n",
    "        param_combos = itertools.product(\n",
    "            *(clf_param_grid[p_name] for p_name in param_names)\n",
    "        )\n",
    "        param_combos_list = list(param_combos)\n",
    "        total_param_combos = len(param_combos_list)\n",
    "\n",
    "        # 3) Loop through parameters\n",
    "        for p_combo_idx in range(total_param_combos):  # Loop through parameters\n",
    "            print(f\"Parameter combination index: {p_combo_idx + 1} out of {total_param_combos}\")\n",
    "            param_combo_ = param_combos_list[p_combo_idx]\n",
    "            model_ = set_model_params(clf_name=clf_idx, params=param_combo_)\n",
    "\n",
    "            # 4) Run CV on the parameter\n",
    "            # Calculate the log-loss for clf_idx-class_idx-p_combo_idx\n",
    "            fold_log_loss_list_ = []\n",
    "            for fold_ in range(5):\n",
    "                run_cv(\n",
    "                    fold=fold_,\n",
    "                    X=X,\n",
    "                    y=y,\n",
    "                    train_idx_list=train_idx_list,\n",
    "                    valid_idx_list=valid_idx_list,\n",
    "                    chosen_class=class_idx,\n",
    "                    fold_log_loss_list = fold_log_loss_list_,\n",
    "                    param_combo_=param_combo_,\n",
    "                    model=model_\n",
    "                )\n",
    "\n",
    "            mean_log_loss = np.mean(fold_log_loss_list_)\n",
    "            result_dict[class_idx][clf_idx].append([mean_log_loss, param_combo_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do a last change to make sure that the final printout includes the parameter combination and not just the relative index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def result_stats(result_dict, clf_list, chosen_classes, param_grid):\n",
    "# For each class print out 1) the best parameter per model 2) the best model/param combo overall\n",
    "\n",
    "stat_dict = {}; best_dict = {} # Initialize dictionaries\n",
    "for class_ in chosen_classes:\n",
    "    temp_clf_dict = {}\n",
    "    for clf_ in clf_list:\n",
    "        temp_clf_dict[clf_] = []\n",
    "    stat_dict[class_] = temp_clf_dict\n",
    "    best_dict[class_] = []\n",
    "\n",
    "# Fill the dictionaries showing best parameters per model (stat_dict)\n",
    "# and best model per class (best_dict)\n",
    "for class_idx in chosen_classes: # Loop through classes\n",
    "    for clf_idx in clf_list: # Loop through classifiers\n",
    "        # Find best parameter (index) per model\n",
    "        temp_class_clf_list = result_dict[class_idx][clf_idx]\n",
    "        cv_score_list = [cv_score[0] for cv_score in temp_class_clf_list]\n",
    "        best_clf_score = min(cv_score_list)\n",
    "        best_idx = cv_score_list.index(best_clf_score)\n",
    "        stat_dict[class_idx][clf_idx] = [temp_class_clf_list[best_idx][1], best_clf_score]\n",
    "\n",
    "    # Find best model/param combo per class\n",
    "    # Reference: https://stackoverflow.com/questions/34249441/finding-minimum-value-in-dictionary\n",
    "    best_clf = min(stat_dict['hidden_class'].items(), key=lambda x: x[1][1])\n",
    "    best_dict[class_idx] = [best_clf]\n",
    "\n",
    "# Reference: https://stackoverflow.com/questions/29771895/save-nested-dictionary-with-differing-number-of-dictionaries    \n",
    "# Save results to csv\n",
    "best_dict_list = [dict(class_name=i, clf_result=j) for i, j in best_dict.items()]\n",
    "with open(\"../output/best_dict.csv\", 'w') as f:\n",
    "    fieldnames = ['class_name', 'clf_result']\n",
    "    w = csv.DictWriter(f, fieldnames)\n",
    "    w.writeheader()\n",
    "    w.writerows(best_dict_list)\n",
    "    \n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "nested_dict_keys = [] # Set fieldnames for stat_dict\n",
    "for idx_i, idx_j in stat_dict.items():\n",
    "    for idx_k, idx_l in idx_j.items():\n",
    "        nested_dict_keys.append(idx_k)\n",
    "nested_dict_keys = list(set(nested_dict_keys))\n",
    "fieldnames = ['class_name'] + nested_dict_keys\n",
    "\n",
    "# Reference: https://stackoverflow.com/questions/29400631/python-writing-nested-dictionary-to-csv\n",
    "# Save results to csv\n",
    "with open(\"../output/stat_dict.csv\", \"w\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames)\n",
    "    w.writeheader()\n",
    "    for key in stat_dict:\n",
    "        w.writerow({field: stat_dict[key].get(field) or key for field in fieldnames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict_csv = pd.read_csv('../output/stat_dict.csv')\n",
    "best_dict_csv = pd.read_csv('../output/best_dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stat_dict_csv['rf'][0].rsplit(', ', 1) \n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dict_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
